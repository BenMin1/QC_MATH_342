\documentclass[12pt]{article}

\include{preamble}

\newtoggle{professormode}
%\toggletrue{professormode} %STUDENTS: DELETE or COMMENT this line



\title{MATH 342W / 642 / RM 742 Spring \the\year~ HW \#4}

\author{Benjamin Minkin} %STUDENTS: write your name here

\iftoggle{professormode}{
\date{Due 11:59PM April 14 \\ \vspace{0.5cm} \small (this document last updated \currenttime~on \today)}
}

\renewcommand{\abstractname}{Instructions and Philosophy}

\begin{document}
\maketitle

\iftoggle{professormode}{
\begin{abstract}
The path to success in this class is to do many problems. Unlike other courses, exclusively doing reading(s) will not help. Coming to lecture is akin to watching workout videos; thinking about and solving problems on your own is the actual ``working out.''  Feel free to \qu{work out} with others; \textbf{I want you to work on this in groups.}

Reading is still \textit{required}. You should be googling and reading about all the concepts introduced in class online. This is your responsibility to supplement in-class with your own readings.

The problems below are color coded: \ingreen{green} problems are considered \textit{easy} and marked \qu{[easy]}; \inorange{yellow} problems are considered \textit{intermediate} and marked \qu{[harder]}, \inred{red} problems are considered \textit{difficult} and marked \qu{[difficult]} and \inpurple{purple} problems are extra credit. The \textit{easy} problems are intended to be ``giveaways'' if you went to class. Do as much as you can of the others; I expect you to at least attempt the \textit{difficult} problems. 

This homework is worth 100 points but the point distribution will not be determined until after the due date. See syllabus for the policy on late homework.

Up to 7 points are given as a bonus if the homework is typed using \LaTeX. Links to instaling \LaTeX~and program for compiling \LaTeX~is found on the syllabus. You are encouraged to use \url{overleaf.com}. If you are handing in homework this way, read the comments in the code; there are two lines to comment out and you should replace my name with yours and write your section. The easiest way to use overleaf is to copy the raw text from hwxx.tex and preamble.tex into two new overleaf tex files with the same name. If you are asked to make drawings, you can take a picture of your handwritten drawing and insert them as figures or leave space using the \qu{$\backslash$vspace} command and draw them in after printing or attach them stapled.

The document is available with spaces for you to write your answers. If not using \LaTeX, print this document and write in your answers. I do not accept homeworks which are \textit{not} on this printout. Keep this first page printed for your records.

\end{abstract}

\thispagestyle{empty}
\vspace{1cm}
NAME: \line(1,0){380}
\clearpage
}


\problem{These are questions about the rest of Silver's book, chapters 7--11. You can skim chapter 10 as it is not so relevant for the class. For all parts in this question, answer using notation from class (i.e. $t ,f, g, h^*, \delta, \epsilon, e, t, z_1, \ldots, z_t, \mathbb{D}, \mathcal{H}, \mathcal{A}, \mathcal{X}, \mathcal{Y}, X, y, n, p, x_{\cdot 1}, \ldots, x_{\cdot p}$, $x_{1 \cdot}, \ldots, x_{n \cdot}$, etc.) as well as in-class concepts (e.g. simulation, validation, overfitting, etc) and also we now have $f_{pr}, h^*_{pr}, g_{pr}, p_{th}$, etc from probabilistic classification as well as different types of validation schemes).

Note: I will not ask questions in this assignment about Bayesian calculations and modeling (a large chunk of Chapters 8 and 10) as this is the subject of Math 341/343. }


\begin{enumerate}

\easysubproblem{Why are flu fatalities hard to predict? Which type of error is most dominant in the models?}\spc{1}

This is because the prediction has an affect on how people act. If the prediction is that many people will die, people will in turn be extra cautious. 

\easysubproblem{In what context does Silver define extrapolation and what term did he use? Why does his terminology conflict with our terminology?}\spc{1}



\easysubproblem{Give a couple examples of extraordinary prediction failures (by vey famous people who were considered heavy-hitting experts of their time) that were due to reckless extrapolations.}\spc{1}

Blockbuster denying the purchase of Netflix comes to mind. They did not think digital streaming would take off like it has. In hindsight this moves seems silly. Another example was IBM predicting that the internet was just a fad. 

\easysubproblem{Using the notation from class, define \qu{self-fulfilling prophecy} and \qu{self-canceling prediction}.}\spc{1}

self-fulfilling prophecy $y = \yhat$ This causes a tautology.
\\self-canceling prediction $y \neq \yhat$ This will never be true. 

\easysubproblem{Is the SIR model of infectious disease under or overfit? Why?}\spc{1}



\easysubproblem{What did the famous mathematician Norbert Weiner mean by \qu{the best model of a cat is a cat}?}\spc{1}

Sometimes there are substitutes that can work better than creating complex models. 

\easysubproblem{Not in the book but about Norbert Weiner. From Wikipedia: 

\begin{quote}
Norbert Wiener is credited as being one of the first to theorize that all intelligent behavior was the result of feedback mechanisms, that could possibly be simulated by machines and was an important early step towards the development of modern artificial intelligence.
\end{quote}

What do we mean by \qu{feedback mechanisms} in the context of this class?}\spc{2}

This can be a model that trains off the same data that it tests on. The feedback loop will make the model think it can predict with a perfect accuracy when in reality it will have poor out of sample performance. 


\easysubproblem{I'm not going to both asking about the bet that gave Bob Voulgaris his start. But what gives Voulgaris an edge (p239)? Frame it in terms of the concepts in this class.}\spc{1}

He was able to separate the bias of following the crowd from his independent thought about the likelihood of success of the lakers. 

\easysubproblem{Why do you think a lot of science is not reproducible?}\spc{1}

I believe that a lot of science is wrong. This is because of reasons including but not limited to innate human biases, Simpsons paradox and the FWER.

\easysubproblem{Why do you think Fisher did not believe that smoking causes lung cancer?}\spc{1}

He enjoyed smoking and was biased. 

\easysubproblem{Is the world moving more in the direction of Fisher's Frequentism or Bayesianism?}\spc{1}

The world is moving more towards using Bayesian methods. 

\easysubproblem{How did Kasparov defeat Deep Blue? Can you put this into the context of over and underfiting?}\spc{1}

Kasparov tried to create complicated positions that forced deep blue into following heuristics. He thought that because he was a better player than the the programmers, his heuristics would reign supreme. 

\easysubproblem{Why was Fischer able to make such bold and daring moves?}\spc{1}



\easysubproblem{What metric $y$ is Google predicting when it returns search results to you? Why did they choose this metric?}\spc{1}

It is predicting what you were searching for.

\easysubproblem{What do we call Google's \qu{theories} in this class? And what do we call \qu{testing} of those theories?}\spc{1}

We called them model predictions. These can be tested with cross validation.

\easysubproblem{p315 give some very practical advice for an aspiring data scientist. There are a lot of push-button tools that exist that automatically fit models. What is your edge from taking this class that you have over people who are well-versed in those tools?}\spc{1}

A lot of fields are zero sum games. This means that having a  strong background can help an aspiring data scientist outperform the competition. 

\easysubproblem{Create your own 2$\times$2 luck-skill matrix (Fig. 10-10) with your own examples (not the ones used in the book).}\spc{2}

\easysubproblem{[EC] Why do you think Billing's algorithms (and other algorithms like his) are not very good at no-limit hold em? I can think of a couple reasons why this would be.}\spc{2}

Poker is a game that has other components aside from math. In some aspects it is more important to play the player instead of playing the game. His model did not have the versatility to play the player so I do not think it would make a good poker player. 

\easysubproblem{Do you agree with Silver's description of what makes people successful (pp326-327)? Explain.}\spc{2}

Yes. This is a type of survivor's bias. 

\easysubproblem{Silver brings up an interesting idea on p328. Should we remove humans from the predictive enterprise completely after a good model has been built? Explain}\spc{2}

No. I do not believe it is ever smart to trust a model completely. It is, by definition, never a perfect representation of reality. 

\easysubproblem{According to Fama, using the notation from this class, how would explain a mutual fund that performs spectacularly in a single year but fails to perform that well in subsequent years?}\spc{1}

This is due to variation. Some models will outperform simply due to luck. We called this estimation error. 

\easysubproblem{Did the Manic Momentum model validate? Explain.}\spc{1}
No. It lost money over time. 
\easysubproblem{Are stock market bubbles noticable while we're in them? Explain.}\spc{1}

Possibly. This is hard to know for certain as people will always be saying that we are in a bubble. Then when one does occur, they claim to have seen it all along. This form of survivorship bias makes this type of prediction hard to assess. 

\easysubproblem{What is the implication of Shiller's model for a long-term investor in stocks?}\spc{1}

In the long run, the only way to beat the market is to have information that hasn't be Incorporated into stock prices. This means that one should invest passively.

\easysubproblem{In lecture one, we spoke about \qu{heuristics} which are simple models with high error but extremely easy to learn and live by. What is the heuristic Silver quotes on p358 and why does it work so well?}\spc{1}

"follow the crowd, especially when we don't know better." This works because groups tend to make better decisions than individuals. This is similar to how averages tend to make better guesses than a random sample of one. 

\easysubproblem{Even if your model at predicting bubbles turned out to be good, what would prevent you from executing on it?}\spc{1}

Some bubbles can last a long time before popping. Even after being identified, the bet against the bubble may become insolvent long before the burst. 

\easysubproblem{How can heuristics get us into trouble?}\spc{5}

Heuristics are just guidelines towards a goal. They are themselves just a means to an end. One should not get this twisted and chase heuristics instead of the goal he is trying to accomplish.

\end{enumerate}


\problem{These are some questions related to polynomial-derived features and logarithm-derived features in use in OLS regression.}

\begin{enumerate}

\intermediatesubproblem{What was the overarching problem we were trying to solve when we started to introduce polynomial terms into $\mathcal{H}$? What was the mathematical theory that justified this solution? Did this turn out to be a good solution? Why / why not?}\spc{3}

The problem we were trying to solve was the issue of mispecification error in linear models. This method increases the candidate space $\mathcal{H}$ to include polynomials which should decrease mispecification error. The justification may be a theorem that supposes all continuous functions can be approximated with polynomials. This turned out to be a decent solution.  

\intermediatesubproblem{We fit the following model: $\yhat = b_0 + b_1 x + b_2 x^2$. What is the interpretation of $b_1$? What is the interpretation of $b_2$? Although we didn't yet discuss the \qu{true} interpretation of OLS coefficients, do your best with this.}\spc{4}

For interpretation, a one unit increase in x is expected to result in a $b_1$ increase in y. Similarly, a one unit increase in $x^2$ is expected to result in a $b_2$ increase in y. 

\hardsubproblem{Assuming the model from the previous question, if $x \in \mathcal{X} = \bracks{10.0, 10.1}$, do you expect to \qu{trust} the estimates $b_1$ and $b_2$? Why or why not?}\spc{7}

No. Based on the \mathcal{X}, I believe using this model for interpretation can be risky. There needs to be a good a-priori reason to trust this model.  

\hardsubproblem{We fit the following model: $\yhat = b_0 + b_1 x_1 + b_2 \natlog{x_2}$. We spoke about in class that $b_1$ represents loosely the predicted change in response for a proportional movement in $x_2$. So e.g. if $x_2$ increases by 10\%, the response is predicted to increase by $0.1 b_2$. Prove this approximation from first principles.}\spc{6}

Yes. This is what I explained above. 

\easysubproblem{When does the approximation from the previous question work? When do you expect the approximation from the previous question not to work?}\spc{4}

This approximation works in most cases however it may not work when x is negative. 

\intermediatesubproblem{We fit the following model: $\natlog{\yhat} = b_0 + b_1 x_1 + b_2 \natlog{x_2}$. What is the interpretation of $b_1$? What is the \emph{approximate} interpretation of $b_2$? Although we didn't yet discuss the \qu{true} interpretation of OLS coefficients, do your best with this.}\spc{2}

A one unit increase in x1 is predicted to increase yhat by ln(b1) units. Similarly, an increase in one unit of x2 is predicted to increase yhat by one unit. 

\easysubproblem{Show that the model from the previous question is equal to $\yhat = m_0 m_1^{x_1} x_2^{b_2}$ and interpret $m_1$.}\spc{2}

To cancel out the ln, put all terms as the power of e. 

\end{enumerate}



\problem{These are some questions related to extrapolation.}

\begin{enumerate}

\easysubproblem{Define extrapolation and describe why it is a net-negative during prediction.}\spc{3}

Extrapolation is the process of attempting to predict on data from processes that were not used to train the model. Extrapolation is harder than interpolation partially because of overfitting and underfitting. It is not easy to ignore the noise and focus on the true relationships in the data. There also may be differing causal drivers between the training data and the extrapolated data.  

\easysubproblem{Do models extrapolate differently? Explain.}\spc{3}

Yes. For example, a linear model predicts on x values infinitely, even when those values are physically impossible. Those x values will likely have equally improbable $\yhat$ values. This is different than the SVM model where new data change change the support vectors.

\easysubproblem{Why do polynomial regression models suffer terribly from extrapolation?}\spc{3}

This is because of the higher dimensional parameters. These can amplify small errors especially in extrapolation. 

\end{enumerate}

%
%\problem{These are some questions related to polynomial-derived features and logarithm-derived features in use in OLS regression.}
%
%\begin{enumerate}
%
%\intermediatesubproblem{What was the overarching problem we were trying to solve when we started to introduce polynomial terms into $\mathcal{H}$? What was the mathematical theory that justified this solution? Did this turn out to be a good solution? Why / why not?}\spc{3}
% the ln, raise all terms to the "e"th power. 
%\intermediatesubproblem{We fit the following model: $\yhat = b_0 + b_1 x + b_2 x^2$. What is the interpretation of $b_1$? What is the interpretation of $b_2$? Although we didn't yet discuss the \qu{true} interpretation of OLS coefficients, do your best with this.}\spc{4}
%
%\hardsubproblem{Assuming the model from the previous question, if $x \in \mathcal{X} = \bracks{10.0, 10.1}$, do you expect to \qu{trust} the estimates $b_1$ and $b_2$? Why or why not?}\spc{7}
%
%\hardsubproblem{We fit the following model: $\yhat = b_0 + b_1 x_1 + b_2 \natlog{x_2}$. We spoke about in class that $b_1$ represents loosely the predicted change in response for a proportional movement in $x_2$. So e.g. if $x_2$ increases by 10\%, the response is predicted to increase by $0.1 b_2$. Prove this approximation from first principles.}\spc{7}
%
%\easysubproblem{When does the approximation from the previous question work? When do you expect the approximation from the previous question not to work?}\spc{2}
%
%\intermediatesubproblem{We fit the following model: $\natlog{\yhat} = b_0 + b_1 x_1 + b_2 \natlog{x_2}$. What is the interpretation of $b_1$? What is the interpretation of $b_2$? Although we didn't yet discuss the \qu{true} interpretation of OLS coefficients, do your best with this.}\spc{3}
%
%\easysubproblem{Show that the model from the previous question is equal to $\yhat = m_0 m_1^{x_1} x_2^{b_2}$ and interpret $m_1$.}\spc{2}
%
%\end{enumerate}

\problem{These are some questions related to the model selection procedure discussed in lecture.}

\begin{enumerate}

\easysubproblem{Define the fundamental problem of \qu{model selection}.}\spc{6}

This problem describes the balancing act between underfitting and overfitting. If the model focuses too much on the training data it may overfit and do poorly in extrapolation. Conversely, if the model doesn't focus enough on the training data, it will also do poorly in extrapolation. 

\easysubproblem{Using two splits of the data, how would you select a model?}\spc{8}

I would train the model on one split of the data and test on the remaining split. Then, I would reverse the roles of the two splits for the next model. 

\easysubproblem{Discuss the main limitation with using two splits to select a model.}\spc{3}

This method will reduce the amount of data being used to train the model at any given time. This will decrease the effectiveness of the model.

\easysubproblem{Using three splits of the data, how would you perform model selection?}\spc{3}

Using three splits, I would train on two splits and test on the remaining data. I would repeat this process three times allowing each split to be used as the testing data. 

\easysubproblem{How does using both inner and outer folds in a double cross-validation nested resampling procedure improve the model selection procedure?}\spc{3}

This method reduces variance as it allows all of the data to be used as both testing data and training data. 

\easysubproblem{Describe how $g_{\text{final}}$ is constructed when using nested resampling on three splits of the data.}\spc{5}

The $g_{\text{final}}$ will result from averaging the three models created from the data. This will create a 'final' model that has less variance than any one of the other models. 

\easysubproblem{Describe how you would use this model selection procedure to find hyperparameter values in algorithms that require hyperparameters.}\spc{3}

I would try many different values for the hyper parameters and keep the one that works the best. 

\hardsubproblem{Given raw features $x_1, \ldots, x_{p_{raw}}$, produce the most expansive set of transformed $p$ features you can think of so that $p \gg n$.}\spc{3}

This is the set of every single group of linear combinations of the raw features. These include exponents, sin waves, etc...

\easysubproblem{Describe the methodology from class that can create a linear model on a subset of the transformed featuers (from the previous problem) that will not overfit.}\spc{10}

Test the functionality of the algorithm before and after adding each new feature to see whether it positively or negatively affected the algorithm. This is done step by step for each feature in a stepwise fashion. 

\end{enumerate}



\problem{These are some questions related to the CART algorithms.}

\begin{enumerate}
\easysubproblem{Write down the step-by-step $\mathcal{A}$ for regression trees.}\spc{7}

Step 1. make a random tree
\\Step 2. at every split point find the best split of the features
\\step 3. take a random subset of the p features

\hardsubproblem{Describe $\mathcal{H}$ for regression trees. This is very difficult but doable. If you can't get it in mathematical form, describe it as best as you can in English.}\spc{7}

This is the function space. It is a set of all "flow charts" that direct towards the classification output.

\intermediatesubproblem{Think of another \qu{leaf assignment} rule besides the average of the responses in the node that makes sense.}\spc{3}

Another possible output can be the mode of the responses.

\intermediatesubproblem{Assume the $y$ values are unique in $\mathbb{D}$. Imagine if $N_0 = 1$ so that each leaf gets one observation and its $\yhat = y_i$ (where $i$ denotes the number of the observation that lands in the leaf) and thus it's very overfit and needs to be \qu{regularized}. Write up an algorithm that finds the optimal tree by pruning one node at a time iteratively. \qu{Prune} means to identify an inner node whose daughter nodes are both leaves and deleting both daughter nodes and converting the inner node into a leaf whose $\yhat$ becomes the average of the responses in the observations that were in the deleted daughter nodes. This is an example of a \qu{backwards stepwise procedure} i.e. the iterations transition from more complex to less complex models.}\spc{5}

Rules:
\\1. If the parent and children node share the same key-value, combine into a single leaf.
\\2. Work from the leaves towards the root and combine in a fashion that minimizes the inaccuracy of the tree. 

\hardsubproblem{Provide an example of an $f(\x)$ relationship with medium noise $\delta$ where vanilla OLS would beat regression trees in oos predictive accuracy. Hint: this is a trick question.}\spc{1}

If within a regression pixel, there is a high variance between the highest and lowest values, the average of that pixel will be inaccurate. 

\easysubproblem{Write down the step-by-step $\mathcal{A}$ for classification trees. This should be short because you can reference the steps you wrote for the regression trees in (a).}\spc{4}


Step 1. make a random tree
\\Step 2. at every split point find the best split of the features
+\\step 3. take a random subset of the p features

The only difference will be that you can't take the mean of classifications and instead will have to return the mode. 


\hardsubproblem{Think of another objective function that makes sense besides the Gini that can be used to compare the \qu{quality} of splits within inner nodes of a classification tree.}\spc{6}

There may be a metric that describes the sections that can classify with perfect accuracy. For example, let accuracyMetric = number of sections that classify perfectly/total. 


\end{enumerate}







\end{document}






