\documentclass[12pt]{article}

\include{preamble}

\newtoggle{professormode}
%\toggletrue{professormode} %STUDENTS: DELETE or COMMENT this line



\title{MATH 342W / 650.4 Spring \the\year~Homework \#2}

\author{Benjamin Minkin} %STUDENTS: write your name here

\iftoggle{professormode}{
\date{Due 11:59PM Feb 25, \the\year \\ \vspace{0.5cm} \small (this document last updated \currenttime~on \today)}
}

\renewcommand{\abstractname}{Instructions and Philosophy}

\begin{document}
\maketitle

\iftoggle{professormode}{
\begin{abstract}
The path to success in this class is to do many problems. Unlike other courses, exclusively doing reading(s) will not help. Coming to lecture is akin to watching workout videos; thinking about and solving problems on your own is the actual ``working out.''  Feel free to \qu{work out} with others; \textbf{I want you to work on this in groups.}

Reading is still \textit{required}. You should be googling and reading about all the concepts introduced in class online. This is your responsibility to supplement in-class with your own readings.

The problems below are color coded: \ingreen{green} problems are considered \textit{easy} and marked \qu{[easy]}; \inorange{yellow} problems are considered \textit{intermediate} and marked \qu{[harder]}, \inred{red} problems are considered \textit{difficult} and marked \qu{[difficult]} and \inpurple{purple} problems are extra credit. The \textit{easy} problems are intended to be ``giveaways'' if you went to class. Do as much as you can of the others; I expect you to at least attempt the \textit{difficult} problems. 

This homework is worth 100 points but the point distribution will not be determined until after the due date. See syllabus for the policy on late homework.

Up to 7 points are given as a bonus if the homework is typed using \LaTeX. Links to instaling \LaTeX~and program for compiling \LaTeX~is found on the syllabus. You are encouraged to use \url{overleaf.com}. If you are handing in homework this way, read the comments in the code; there are two lines to comment out and you should replace my name with yours and write your section. The easiest way to use overleaf is to copy the raw text from hwxx.tex and preamble.tex into two new overleaf tex files with the same name. If you are asked to make drawings, you can take a picture of your handwritten drawing and insert them as figures or leave space using the \qu{$\backslash$vspace} command and draw them in after printing or attach them stapled.

The document is available with spaces for you to write your answers. If not using \LaTeX, print this document and write in your answers. I do not accept homeworks which are \textit{not} on this printout. Keep this first page printed for your records.

\end{abstract}

\thispagestyle{empty}
\vspace{1cm}
NAME: \line(1,0){380}
\clearpage
}

\problem{These are questions about Silver's book, chapter 2, 3.  Answer the questions using notation from class (i.e. $t ,f, g, h^*, \delta, \epsilon, e, t, z_1, \ldots, z_t, \mathbb{D}, \mathcal{H}, \mathcal{A}, \mathcal{X}, \mathcal{Y}, X, y, n, p, x_{\cdot 1}, \ldots, x_{\cdot p}, x_{1 \cdot}, \ldots, x_{n \cdot}$, etc).}


\begin{enumerate}

\intermediatesubproblem{If one's goal is to fit a model for a phenomenon $y$, what is the difference between the approaches of the hedgehog and the fox? Connecting this to the modeling framework should really make you think about what Tetlock's observation means for political and historical phenomena.}\spc{3}

The hedgehog would try to fit the model towards his broad ideas about how the world functions. This would create a biased model that seeks to confirm his biases. The fox however, would try to understand the phenomenon as a unique process and would create a less biased model but without a point of reference. 

\easysubproblem{Why did Harry Truman like hedgehogs? Are there a lot of people that think this way?}\spc{2}

Most people like hedgehogs. They claim to understand everything and can explain historical events from a simplistic perspective. Not many people want to understand history in a nuanced approach that describes events as a result of a multitude of influences.

\hardsubproblem{Why is it that the more education one acquires, the less accurate one's predictions become?}\spc{3}

This is not necessarily true but is often the case because of how education is run. Students inherit the beliefs and biases of the teacher. This creates underlying presumptions that can hinder the creation of unbiased models. 

\easysubproblem{Why are probabilistic classifiers (i.e. algorithms that output functions that return probabilities) better than vanilla classifiers (i.e. algorithms that only return the class label)? We will move in this direction in class soon.}\spc{4}

This approach allows for better back-testing of models. One can look back and see how accurate the predictions were. A good probabilistic model will be right at the predicted rate. This is better than vanilla classifiers where it is harder to do so. 

\easysubproblem{What algorithm that we studied in class is PECOTA most similar to?}\spc{1}

This is similar to the nearest k neighbors algorithm. 

\easysubproblem{Is baseball performance as a function of age a linear model? Discuss.}\spc{4}

No. It is presumed that players get better with experience and worse with age. This leads to a peak at some point in the middle of their career. Psychologically, this makes sense. A good player often never quits at his physical peak and usually will retire after he can no longer play at his best. 

\intermediatesubproblem{How can baseball scouts do better than a prediction system like PECOTA?}\spc{6}

They can watch the players play. This firsthand view along with years of experience can cause scouts to have a better prediction.

\intermediatesubproblem{Why hasn't anyone (at the time of the writing of Silver's book) taken advantage of Pitch f/x data to predict future success?}\spc{6}

There are lots of stats recorded in baseball. Pitch f/x is just one that is collected. It is hard to understand which statistics are meaningful and which are noise.  

\end{enumerate}



\problem{These are questions about the SVM.}

\begin{enumerate}

\easysubproblem{State the hypothesis set $\mathcal{H}$ inputted into the support vector machine algorithm. Is it different than the $\mathcal{H}$ used for $\mathcal{A}$ = perceptron learning algorithm?}\spc{1}

$$\mathcal{A} = \braces{\indic {w \cdot x}-b \leq 0 : w \in \reals, b \in \reals}$$
This is limited to a line/hyperplane that splits the wedge near the center. This is different than perceptron method as that will output any valid line in the wedge.

\extracreditsubproblem{Prove the max-margin linearly separable SVM converges. State all assumptions. Write it on a separate page.}\spc{-0.5}

\hardsubproblem{Let $\mathcal{Y} = \braces{-1,1}$. Rederive the cost function whose minimization yields the SVM line in the linearly separable case. }\spc{20}
$$L_i(w,b) = max(0,1-y_i(f(x_i)))$$
or:
$$\frac{1}{2}||w||^2$$

\easysubproblem{Given your answer to (c) rederive the cost function using the \qu{soft margin} i.e. the hinge loss plus the term with the hyperparameter $\lambda$. This is marked easy since there is just one change from the expression given in class.}\spc{4}

$$L_i(w,b) = max(0,1-y_i(f(x_i))+\epsilon)$$
or:
$$\frac{1}{2}||w||^2 + \epsilon$$

\end{enumerate}



\problem{These are questions are about the $k$ nearest neighbors (KNN) algorithm.}

\begin{enumerate}

\easysubproblem{Describe how the algorithm works. Is $k$ a \qu{hyperparameter}?}\spc{5}

This algorithm predicts an output based on the closest k matches in the dataset. k is a hyperparameter as it is a adjusted to yield the best results.  

\hardsubproblem{[MA] Assuming $\mathcal{A} = $ KNN, describe the input $\mathcal{H}$ as best as you can.}\spc{8}

\easysubproblem{When predicting on $\mathbb{D}$ with $k=1$, why should there be zero error? Is this a good estimate of future error when new data comes in? (Error in the future is called \emph{generalization error} and we will be discussing this later in the semester).}\spc{5}

When predicting on the dataset with k=1, you will have 0 error. This is because the nearest match in the dataset will be that exact value. However, this will not hold in future data or when k is greater than 1. 

\end{enumerate}

\problem{These are questions about the linear model with $p=1$.}

\begin{enumerate}

\easysubproblem{What does $\mathbb{D}$ look like in the linear model with $p=1$? What is $\mathcal{X}$? What is $\mathcal{Y}$?}\spc{3}

$\mathbb{D}$ is n different points with one input and output. 
\\$\mathcal{X}$ are the inputs while
\\$\mathcal{Y}$ are the outputs

\easysubproblem{Consider the line fit using the ordinary least squares (OLS) algorithm. Prove that the point $<\xbar, \ybar>$ is on this line. Use the formulas we derived in class.}\spc{3}

$$b_0 = \ybar - b_1\xbar$$
\\
$$b_1 = \frac{\sum_1^n (x_i - \xbar)(y_i - \ybar)}{\sum_1^n (x_i - \xbar)^2}$$
Setting $b_1$ = 0 yields the line:
$$b_0 = \ybar - 0\xbar = \ybar \rightarrow y = \ybar + (0) x$$
This passes through $<\ybar,\xbar>$


\intermediatesubproblem{Consider the line fit using OLS. Prove that the average prediction $\hat{y}_i := g(x_i)$ for $x_i \in \mathbb{D}$ is $\ybar$.}\spc{4}

$$e = y_i - \ybar$$
$$e^{bar} = 0 \rightarrow mean(y_i - \ybar) = 0 \rightarrow mean(y_i) = \ybar$$

\intermediatesubproblem{Consider the line fit using OLS. Prove that the average residual $e_i$ is 0 over $\mathbb{D}$.}\spc{6}

The average can be defined as:
$$e^{bar} = \frac{1}{n}* \sum_i^n{e}$$
$$\sum_i^n{e} = 0 \rightarrow  e^{bar} = 0$$

\intermediatesubproblem{Why is the RMSE usually a better indicator of predictive performance than $R^2$? Discuss in English.}\spc{4}

RMSE clearly outlines the bounds defining the variance. This is more clear than $ R^2$ which can be high but with lots of variance remaining. 


\intermediatesubproblem{$R^2$ is commonly interpreted as \qu{proportion of the variance explained by the model} and proportions are constrained to the interval $\zeroonecl$. While it is true that $R^2 \leq 1$ for all models, it is not true that $R^2 \geq 0$ for all models. Construct an explicit example $\mathbb{D}$ and create a linear model $g(x) = w_0 + w_1 x$ whose $R^2 < 0$.}\spc{10}

This can be true if the null model has less variance than the linear model. This is possible when there are a few outliers that greatly shift the line.


\hardsubproblem{You are given $\mathbb{D}$ with $n$ training points $<x_i, y_i>$ but now you are also given a set of weights $\bracks{w_1~w_2~ \ldots ~w_n}$ which indicate how costly the error is for each of the $i$ points. Rederive the least squares estimates $b_0$ and $b_1$ under this situation. Note that these estimates are called the \emph{weighted least squares regression} estimates. This variant $\mathcal{A}$ on OLS has a number of practical uses, especially in Economics. No need to simplify your answers like I did in class (i.e. you can leave in ugly sums).}\spc{12.5}

$$\mathbb{D} = \sum_i^n{ w_i(y_i - b_0 - b_1x_i)^2}$$
Then take the partial derivative of $b_0$ and $b_1$ respectively:
$$\frac{d}{d(b_0)} = -2 * \sum_i^n {w_i(y_i - b_0 - b_1x_i)}$$
$$\frac{d}{d(b_1)} = -2 * \sum_i^n {w_i(y_i - b_0 - b_1x_i) * (x_i)}$$

\intermediatesubproblem{Interpret the ugly sums in the $b_0$ and $b_1$ you derived above and compare them to the $b_0$ and $b_1$ estimates in OLS. Does it make sense each term should be altered in this matter given your goal in the weighted least squares?}\spc{5}


%\hardsubproblem{[MA] In class we talked about $x_{raw} \in \braces{\text{red}, \text{green}}$ and the OLS model was the sample average of the inputted $x$ where $b_0 = \ybar_r$ and $b_1 = \ybar_g - \ybar_r$. Reparameterize $\mathcal{H} = \braces{w_1\indic{x_{raw} =~\text{red}}  + w_2 \indic{x_{raw} =~\text{green}}~:~ w_1, w_2 \in \reals}$ and prove that the OLS estimates are $b_1 = \ybar_r$ and $b_2 = \ybar_g$.}\spc{20}

\extracreditsubproblem{In class we talked about $x_{raw} \in \braces{\text{red}, \text{green}}$ and the OLS model was the sample average of the inputted $x$. Imagine if you have the additional constraint that $x_{raw}$ is ordinal e.g. $x_{raw} \in \braces{\text{low}, \text{high}}$ and you were forced to have a model where $g$(low) $\leq$ $g$(high). Write about an algorithm $\mathcal{A}$ that can solve this problem.}\spc{10}

\end{enumerate}

\problem{These are questions about association and correlation.}


\begin{enumerate}

\easysubproblem{Give an example of two variables that are both correlated and associated by drawing a plot.}\spc{7}
\includegraphics[]{342wcorandassoc.PNG}

\easysubproblem{Give an example of two variables that are not correlated but are associated by drawing a plot.}\spc{7}
\includegraphics[]{math 342w nocoryesasoc.PNG}
\easysubproblem{Give an example of two variables that are not correlated nor associated by drawing a plot.}\spc{7}
\includegraphics[]{math342nocororasoc.PNG}
\easysubproblem{Can two variables be correlated but not associated? Explain.}\spc{7}

No. Everything that is correlated is associated. Correlation is just one type of (linear) association. 

\end{enumerate}


\problem{These are questions about multivariate linear model fitting using the least squares algorithm.}

\begin{enumerate}

\hardsubproblem{Derive $\partialop{\c}{\c^\top A \c}$ where $\c \in \reals^n$ and $A \in \reals^{n \times n}$ but \textit{not} symmetric. Get as far as you can.}\spc{8}

\easysubproblem{Given matrix $X \in \reals^{n \times (p+1)}$, full rank and first column consisting of the $\onevec_n$ vector, rederive the least squares solution $\b$ (the vector of coefficients in the linear model shipped in the prediction function $g$). No need to rederive the facts about vector derivatives.}\spc{10}

\intermediatesubproblem{Consider the case where $p = 1$. Show that the solution for $\b$ you just derived in (b) is the same solution that we proved for simple regression. That is, the first element of $\b$ is the same as $b_0 = \ybar - r \frac{s_y}{s_x}\xbar$ and the second element of $\b$ is $b_1 = r \frac{s_y}{s_x}$.} \spc{7}

Where p=1, the x matrix will be a 1xn column vector like before.

\easysubproblem{If $X$ is rank deficient, how can you solve for $\b$? Explain in English.} \spc{2}

The first step would be to identify and remove the linearly dependant columns. Then proceed with full rank. 


\hardsubproblem{Prove $\rank{X} =\rank{X^\top X}$.}\spc{9}

columns = nullity + rank
\\Assume x has nullity of 0. nullity $X^\top X = 0 $ $$\rank{X} =\rank{X^\top X}$$

%\hardsubproblem{Given matrix $X \in \reals^{n \times (p+1)}$, full rank and first column consisting of the $\onevec_n$ vector, now consider cost multiples (\qu{weights}) $c_1, c_2, \ldots, c_n$ for each mistake $e_i$. As an example, previously the mistake for the 17th observation was $e_{17} := y_{17} - \hat{y}_{17}$ but now it would be $e_{17} := c_{17} (y_{17} - \hat{y}_{17})$.  Derive the weighted least squares solution $\b$. No need to rederive the facts about vector derivatives. Hints: (1) show that SSE is a quadratic form with the matrix $C$ in the middle (2) Split this matrix up into two pieces i.e. $C = C^{\half} C^{\half}$, distribute and then foil (3) note that a scalar value equals its own transpose and (4) use the vector derivative formulas.}\spc{10}


\intermediatesubproblem{[MA] If $p=1$, prove $r^2 = R^2$ i.e. the linear correlation is the same as proportion of sample variance explained in a least squares linear model.}\spc{6}

\intermediatesubproblem{Prove that $g(\bracks{1 ~\xbar_1~ \xbar_2~ \ldots~ \xbar_p}) =\bar{y}$ in OLS.}\spc{7}

$$e = y_i - \ybar$$
$$e^{bar} = 0 \rightarrow mean(y_i - \ybar) = 0 \rightarrow mean(y_i) = \ybar$$

\intermediatesubproblem{Prove that $\bar{e} = 0$ in OLS.}\spc{10}

The average can be defined as:
$$e^{bar} = \frac{1}{n}* \sum_i^n{e}$$
$$\sum_i^n{e} = 0 \rightarrow  e^{bar} = 0$$

\hardsubproblem{If you model $\y$ with one categorical nominal variable that has levels $A, B, C$, prove that the OLS estimates look like $\ybar_A$ if $x = A$, $\ybar_B$ if $x = B$ and $\ybar_C$ if $x = C$. You can choose to use an intercept or not. Likely without is easier.}\spc{10}

$$(\ybar|x=A) = A$$
$$(\ybar|x=B) = B$$
$$(\ybar|x=C) = C$$
\intermediatesubproblem{[MA] Prove that the OLS model always has $R^2 \in \zeroonecl$.}\spc{5}

\end{enumerate}



\end{document}

